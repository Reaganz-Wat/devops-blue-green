#!/usr/bin/env python3
"""
Nginx Log Watcher for Blue/Green Deployment
Monitors Nginx access logs and sends Slack alerts for:
- Pool failover events (Blue <-> Green)
- High upstream error rates
"""

import os
import sys
import time
import re
import json
import subprocess
from collections import deque
from datetime import datetime, timedelta
import requests


class LogWatcher:
    def __init__(self):
        # Environment variables
        self.slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
        self.error_threshold = float(os.getenv('ERROR_RATE_THRESHOLD', '2'))
        self.window_size = int(os.getenv('WINDOW_SIZE', '200'))
        self.cooldown_sec = int(os.getenv('ALERT_COOLDOWN_SEC', '300'))
        self.maintenance_mode = os.getenv('MAINTENANCE_MODE', 'false').lower() == 'true'
        
        # State tracking
        self.last_pool = None
        self.request_window = deque(maxlen=self.window_size)
        self.last_failover_alert = None
        self.last_error_alert = None
        self.last_recovery_alert = None
        
        # Validate configuration
        if not self.slack_webhook or 'YOUR/WEBHOOK/URL' in self.slack_webhook:
            print("‚ö†Ô∏è  WARNING: SLACK_WEBHOOK_URL not configured. Alerts will be printed to console only.")
            self.slack_webhook = None
        
        print(f"üöÄ Log Watcher Started")
        print(f"   Error Threshold: {self.error_threshold}%")
        print(f"   Window Size: {self.window_size} requests")
        print(f"   Cooldown: {self.cooldown_sec}s")
        print(f"   Maintenance Mode: {self.maintenance_mode}")
        print(f"   Watching: Docker logs from nginx_proxy")
        print("-" * 60)

    def parse_log_line(self, line):
        """Parse Nginx log line to extract pool, status, and upstream info."""
        try:
            # Skip lines without pool info
            if 'pool=' not in line:
                return None
            
            # Extract pool (e.g., pool=blue or pool=green)
            pool_match = re.search(r'pool=(\w+)', line)
            pool = pool_match.group(1) if pool_match else None
            
            # Extract upstream status (e.g., upstream_status=200 or 500)
            # Handle multiple statuses like "500, 200"
            status_match = re.search(r'upstream_status=([\d,\s]+)', line)
            if status_match:
                statuses = status_match.group(1).split(',')
                # Take the first status (the one that failed)
                upstream_status = int(statuses[0].strip())
            else:
                upstream_status = None
            
            # Extract upstream address
            addr_match = re.search(r'upstream_addr=([\d\.:,\s]+)', line)
            upstream_addr = addr_match.group(1).strip() if addr_match else None
            
            # Extract request time
            req_time_match = re.search(r'request_time=([\d\.]+)', line)
            request_time = float(req_time_match.group(1)) if req_time_match else None
            
            return {
                'pool': pool,
                'upstream_status': upstream_status,
                'upstream_addr': upstream_addr,
                'request_time': request_time,
                'timestamp': datetime.now()
            }
        except Exception as e:
            # Don't print errors for every line, just return None
            return None

    def send_slack_alert(self, message, color="warning", alert_type="info"):
        """Send alert to Slack webhook."""
        if self.maintenance_mode:
            print(f"üîß MAINTENANCE MODE: Suppressing {alert_type} alert")
            return
        
        payload = {
            "attachments": [{
                "color": color,
                "title": f"üö® Blue/Green Deployment Alert",
                "text": message,
                "footer": "Nginx Log Watcher",
                "ts": int(time.time())
            }]
        }
        
        # Print to console
        print(f"\n{'='*60}")
        print(f"üì¢ ALERT [{alert_type.upper()}]: {message}")
        print(f"{'='*60}\n")
        
        # Send to Slack if configured
        if self.slack_webhook:
            try:
                response = requests.post(
                    self.slack_webhook,
                    json=payload,
                    headers={'Content-Type': 'application/json'},
                    timeout=10
                )
                if response.status_code == 200:
                    print(f"‚úÖ Alert sent to Slack successfully")
                else:
                    print(f"‚ö†Ô∏è  Slack webhook returned status {response.status_code}")
            except Exception as e:
                print(f"‚ùå Failed to send Slack alert: {e}")

    def check_failover(self, current_pool):
        """Detect pool failover events."""
        if self.last_pool is None:
            self.last_pool = current_pool
            print(f"üìç Initial pool detected: {current_pool}")
            return
        
        if current_pool != self.last_pool and current_pool is not None:
            # Cooldown check
            now = datetime.now()
            if self.last_failover_alert:
                elapsed = (now - self.last_failover_alert).total_seconds()
                if elapsed < self.cooldown_sec:
                    print(f"‚è≥ Failover cooldown active ({int(elapsed)}s / {self.cooldown_sec}s)")
                    return
            
            # Failover detected!
            message = (
                f"*Failover Detected*\n"
                f"Pool switched from *{self.last_pool}* ‚Üí *{current_pool}*\n"
                f"Time: {now.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                f"*Action Required:*\n"
                f"‚Ä¢ Check health of `{self.last_pool}` container\n"
                f"‚Ä¢ Review logs: `docker logs app_{self.last_pool}`\n"
                f"‚Ä¢ Verify application is recovering"
            )
            
            self.send_slack_alert(message, color="danger", alert_type="failover")
            self.last_failover_alert = now
            self.last_pool = current_pool

    def check_error_rate(self):
        """Calculate and alert on high error rates."""
        if len(self.request_window) < min(50, self.window_size):
            return  # Not enough data yet
        
        # Count 5xx errors
        error_count = sum(1 for req in self.request_window 
                         if req['upstream_status'] and req['upstream_status'] >= 500)
        
        total_count = len(self.request_window)
        error_rate = (error_count / total_count) * 100
        
        if error_rate > self.error_threshold:
            # Cooldown check
            now = datetime.now()
            if self.last_error_alert:
                elapsed = (now - self.last_error_alert).total_seconds()
                if elapsed < self.cooldown_sec:
                    return
            
            # High error rate detected!
            message = (
                f"*High Error Rate Detected*\n"
                f"Error Rate: *{error_rate:.2f}%* (threshold: {self.error_threshold}%)\n"
                f"Errors: {error_count} / {total_count} requests\n"
                f"Window: Last {self.window_size} requests\n"
                f"Time: {now.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                f"*Action Required:*\n"
                f"‚Ä¢ Check upstream application logs\n"
                f"‚Ä¢ Consider toggling to backup pool\n"
                f"‚Ä¢ Investigate root cause of 5xx errors"
            )
            
            self.send_slack_alert(message, color="danger", alert_type="error_rate")
            self.last_error_alert = now
        
        # Check for recovery
        elif error_rate < (self.error_threshold / 2) and self.last_error_alert:
            now = datetime.now()
            elapsed = (now - self.last_error_alert).total_seconds()
            
            if 60 < elapsed < self.cooldown_sec * 2:
                if not self.last_recovery_alert or \
                   (now - self.last_recovery_alert).total_seconds() > self.cooldown_sec:
                    
                    message = (
                        f"*Service Recovery*\n"
                        f"Error rate has improved: *{error_rate:.2f}%*\n"
                        f"System is stabilizing\n"
                        f"Time: {now.strftime('%Y-%m-%d %H:%M:%S')}"
                    )
                    
                    self.send_slack_alert(message, color="good", alert_type="recovery")
                    self.last_recovery_alert = now

    def tail_docker_logs(self):
        """Tail Docker logs from nginx_proxy container."""
        print(f"‚è≥ Connecting to nginx_proxy container logs...")
        
        try:
            # Use docker logs -f to follow the nginx container logs
            process = subprocess.Popen(
                ['docker', 'logs', '-f', '--tail', '0', 'nginx_proxy'],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,  # Nginx logs go to stderr
                universal_newlines=True,
                bufsize=1
            )
            
            print(f"‚úÖ Connected to nginx_proxy logs")
            print(f"üìä Monitoring started. Waiting for traffic...\n")
            
            for line in iter(process.stdout.readline, ''):
                if not line:
                    continue
                
                line = line.strip()
                if not line:
                    continue
                
                # Parse log line
                parsed = self.parse_log_line(line)
                if not parsed or not parsed['pool']:
                    continue
                
                # Add to window
                self.request_window.append(parsed)
                
                # Check for failover
                self.check_failover(parsed['pool'])
                
                # Check error rate
                self.check_error_rate()
                
        except KeyboardInterrupt:
            process.terminate()
            raise
        except Exception as e:
            print(f"‚ùå Error in tail process: {e}")
            if 'process' in locals():
                process.terminate()
            raise

    def run(self):
        """Main run loop."""
        try:
            self.tail_docker_logs()
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Watcher stopped by user")
            sys.exit(0)
        except Exception as e:
            print(f"‚ùå Fatal error: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


if __name__ == '__main__':
    watcher = LogWatcher()
    watcher.run()